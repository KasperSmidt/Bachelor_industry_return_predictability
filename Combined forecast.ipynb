{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eedc98-a040-46e0-a378-034fe423352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reduce display precision on numpy arrays\n",
    "np.set_printoptions(precision=5)\n",
    "\n",
    "df = pd.read_csv('C:/Users/Computer/Documents/bachelor/dataset.csv')\n",
    "pd.options.mode.chained_assignment=None\n",
    "df['csp'][637:]  = df['csp'][0:638].mean()\n",
    "\n",
    "df['Index'] = pd.to_numeric(df['Index'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f56f5-aad2-4a30-a924-00c6ef7201a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784137d4-2a64-476c-ac22-abe092d9ec0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-30].values\n",
    "Y = df.iloc[:, -30:].values\n",
    "\n",
    "feature_names = df.columns[1:18]\n",
    "target_names = df.columns[-30:] \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_df = df[feature_names]\n",
    "Y_df = df[target_names]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_df = pd.DataFrame(scaler.fit_transform(X_df), columns=X_df.columns)\n",
    "\n",
    "data = pd.concat([Y_df, X_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abac444-44ee-4d31-b7b1-a41d4cfd5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV, LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def lasso_feature_selection(train, test, y_key, model_config_ols, jobs=24):\n",
    "    time_series_cv = TimeSeriesSplit(n_splits=model_config_ols[\"tscv_splits\"])\n",
    "    lasso = LassoCV(n_alphas=model_config_ols[\"n_alphas\"], cv=time_series_cv, n_jobs=jobs, random_state=42, max_iter=20000)\n",
    "    lasso.fit(train.drop(y_key, axis=1), train[y_key])\n",
    "    \n",
    "    selected_features_ols = train.drop(y_key, axis=1).columns[lasso.coef_ != 0]\n",
    "    return selected_features_ols\n",
    "\n",
    "def ols(train, test, y_key, selected_features_ols):\n",
    "    X_train = train[selected_features_ols]\n",
    "    X_test = test[selected_features_ols]\n",
    "    y_train = train[y_key]\n",
    "    \n",
    "    ols_model = LinearRegression().fit(X_train, y_train)\n",
    "    y_hat_ols = pd.Series(ols_model.predict(X_test), index=test.index).rename(\"ols_post_lasso_y_hat\")\n",
    "    \n",
    "    return y_hat_ols\n",
    "\n",
    "def multioutput_ols_post_lasso(train, test, y_keys, model_config_ols, jobs=24):\n",
    "    preds_ols = {}\n",
    "    for y_key in y_keys:\n",
    "        selected_features_ols = lasso_feature_selection(train, test, y_key, model_config_ols, jobs)\n",
    "        preds_ols[y_key] = ols(train, test, y_key, selected_features_ols)\n",
    "    return preds_ols\n",
    "\n",
    "model_config_ols = {\n",
    "    \"n_alphas\": 100,\n",
    "    \"tscv_splits\": 20\n",
    "}\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "train_start_size = 200\n",
    "\n",
    "# Initialize empty dictionaries to store results\n",
    "Actuals_ols = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Forecasts_ols = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Forecast_Errors_ols = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "MAE_ols = {}\n",
    "MSE_ols = {}\n",
    "\n",
    "# Expanding window loop\n",
    "for t in range(train_start_size, len(data) - 1):\n",
    "    train = data.iloc[:t]\n",
    "    test = data.iloc[t : t + 1]\n",
    "\n",
    "    preds_ols = multioutput_ols_post_lasso(train, test, Y_df.columns, model_config_ols, jobs=10)\n",
    "\n",
    "    for col in Y_df.columns:\n",
    "        Forecasts_ols[col] = pd.concat([Forecasts_ols[col], preds_ols[col]])\n",
    "        Actuals_ols[col] = pd.concat([Actuals_ols[col], test[col]])\n",
    "        Forecast_Errors_ols[col] = pd.concat([Forecast_Errors_ols[col], test[col] - preds_ols[col]])\n",
    "        \n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(data) - 1}\")\n",
    "\n",
    "# Calculate MAE and MSE for each target variable\n",
    "for col in Y_df.columns:\n",
    "    MAE_ols[col] = mean_absolute_error(Actuals_ols[col], Forecasts_ols[col])\n",
    "    MSE_ols[col] = mean_squared_error(Actuals_ols[col], Forecasts_ols[col])\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    print(f\"Actuals for {col}:\")\n",
    "    print(Actuals_ols[col])\n",
    "    print(f\"Forecasts for {col}:\")\n",
    "    print(Forecasts_ols[col])\n",
    "    print(f\"Forecast Errors for {col}:\")\n",
    "    print(Forecast_Errors_ols[col])\n",
    "    print(f\"Mean Absolute Error for {col}: {MAE_ols[col]}\")\n",
    "    print(f\"Mean Squared Error for {col}: {MSE_ols[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188cfb75-5749-4afa-9690-87f8132fa1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75520311-ff42-4a51-934b-326eac367e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff95fe-ced2-4eda-8751-20f1eced8845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def create_nn(hidden_layers, neurons, dropout_rate, learning_rate, alpha):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=17, kernel_initializer='normal', activation='relu', kernel_regularizer='l2'))\n",
    "\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        model.add(Dense(neurons, activation='relu', kernel_regularizer='l2'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(30, activation='linear'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "def initial_hyperparameter_tuning(train, y_keys, model_config_ffn):\n",
    "    X_train = train.drop(y_keys, axis=1)\n",
    "    y_train = train[y_keys]\n",
    "\n",
    "    time_series_cv = TimeSeriesSplit(n_splits=model_config_ffn[\"tscv_splits\"])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    param_grid_ffn = {\n",
    "        \"hidden_layers\": [1, 2, 3],\n",
    "        \"neurons\": [16, 32, 64],\n",
    "        \"dropout_rate\": [0.1, 0.2],\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"alpha\": [0.01, 0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    grid_search_ffn = GridSearchCV(KerasRegressor(model=create_nn, epochs=100, batch_size=10, verbose=0, alpha=0.01, learning_rate=0.01, dropout_rate=0.1, neurons=16, hidden_layers=1),\n",
    "                               param_grid_ffn, cv=time_series_cv, scoring=\"neg_mean_squared_error\", n_jobs=1)\n",
    "    grid_search_ffn.fit(X_train_scaled, y_train)\n",
    "    best_params_ffn = grid_search_ffn.best_params_\n",
    "\n",
    "    return best_params_ffn\n",
    "\n",
    "def nn_tuned(train, test, y_keys, best_params):\n",
    "    X_train = train.drop(y_keys, axis=1)\n",
    "    X_test = test.drop(y_keys, axis=1)\n",
    "    y_train = train[y_keys]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    model = create_nn(best_params_ffn[\"hidden_layers\"], best_params_ffn[\"neurons\"], best_params_ffn[\"dropout_rate\"],\n",
    "                      best_params_ffn[\"learning_rate\"], best_params_ffn[\"alpha\"])\n",
    "    model.fit(X_train_scaled, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "    y_hat_ffn = pd.DataFrame(model.predict(X_test_scaled), index=test.index, columns=y_keys)\n",
    "\n",
    "    return y_hat_ffn\n",
    "\n",
    "\n",
    "model_config_ffn = {\n",
    "    \"tscv_splits\": 20\n",
    "}\n",
    "\n",
    "\n",
    "# Perform hyperparameter tuning on the initial training set\n",
    "train_start_size = 200\n",
    "initial_train = data.iloc[:train_start_size]\n",
    "\n",
    "best_params_ffn = initial_hyperparameter_tuning(initial_train, Y_df.columns, model_config_ffn)\n",
    "\n",
    "Actuals_ffn = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Forecasts_ffn = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Forecast_Errors_ffn = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "MAE_ffn = {}\n",
    "MSE_ffn = {}\n",
    "\n",
    "# Expanding window loop\n",
    "for t in range(train_start_size, len(data) - 1):\n",
    "    train = data.iloc[:t]\n",
    "    test = data.iloc[t : t + 1]\n",
    "\n",
    "    preds_ffn = nn_tuned(train, test, Y_df.columns, best_params_ffn)\n",
    "\n",
    "    for col in Y_df.columns:\n",
    "        Forecasts_ffn[col] = pd.concat([Forecasts_ffn[col], preds_ffn[col]])\n",
    "        Actuals_ffn[col] = pd.concat([Actuals_ffn[col], test[col]])\n",
    "        Forecast_Errors_ffn[col] = pd.concat([Forecast_Errors_ffn[col], test[col] - preds_ffn[col]])\n",
    "\n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(data) - 1}\")\n",
    "\n",
    "# Calculate MAE and MSE for each target variable\n",
    "for col in Y_df.columns:\n",
    "    MAE_ffn[col] = mean_absolute_error(Actuals_ffn[col], Forecasts_ffn[col])\n",
    "    MSE_ffn[col] = mean_squared_error(Actuals_ffn[col], Forecasts_ffn[col])\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    print(f\"Actuals for {col}:\")\n",
    "    print(Actuals_ffn[col])\n",
    "    print(f\"Forecasts for {col}:\")\n",
    "    print(Forecasts_ffn[col])\n",
    "    print(f\"Forecast Errors for {col}:\")\n",
    "    print(Forecast_Errors_ffn[col])\n",
    "    print(f\"Mean Absolute Error for {col}: {MAE_ffn[col]}\")\n",
    "    print(f\"Mean Squared Error for {col}: {MSE_ffn[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e88970-abca-4c09-848d-31c17da8ae49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719d357-0beb-4f5d-8e11-3a547f4d7b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def extratrees_feature_selection(train, y_key, jobs=24):\n",
    "    X_train = train.drop(y_key, axis=1)\n",
    "    y_train = train[y_key]\n",
    "\n",
    "    extratrees = ExtraTreesRegressor(n_jobs=jobs, random_state=42)\n",
    "    extratrees.fit(X_train, y_train)\n",
    "\n",
    "    # Select features using ExtraTreesRegressor\n",
    "    selector = SelectFromModel(extratrees, prefit=True)\n",
    "    selected_features_svr = train.drop(y_key, axis=1).columns[selector.get_support()]\n",
    "    \n",
    "    return selected_features_svr\n",
    "\n",
    "def initial_hyperparameter_tuning(train, y_key, selected_features_svr, model_config_svr):\n",
    "    X_train = train[selected_features_svr]\n",
    "    y_train = train[y_key]\n",
    "\n",
    "    time_series_cv = TimeSeriesSplit(n_splits=model_config_svr[\"tscv_splits\"])\n",
    "    \n",
    "    pipeline = Pipeline([(\"svr\", SVR())])\n",
    "    param_grid_svr = {\n",
    "        \"svr__kernel\": [\"linear\", \"rbf\"],\n",
    "        \"svr__C\": [0.1, 1, 10],\n",
    "        \"svr__epsilon\": [0.01, 0.1, 1, 2]\n",
    "    }\n",
    "    \n",
    "    grid_search_svr = GridSearchCV(pipeline, param_grid_svr, cv=time_series_cv, scoring=\"neg_mean_squared_error\", n_jobs=24)\n",
    "    grid_search_svr.fit(X_train, y_train)\n",
    "    best_params_svr = grid_search_svr.best_params_\n",
    "    \n",
    "    return best_params_svr\n",
    "\n",
    "def svr_tuned(train, test, y_key, selected_features_svr, best_params_svr):\n",
    "    X_train = train[selected_features_svr]\n",
    "    X_test = test[selected_features_svr]\n",
    "    y_train = train[y_key]\n",
    "\n",
    "    svr_model = SVR(kernel=best_params_svr[y_key][\"svr__kernel\"], C=best_params_svr[y_key][\"svr__C\"], epsilon=best_params_svr[y_key][\"svr__epsilon\"])\n",
    "    svr_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_hat_svr = pd.Series(svr_model.predict(X_test), index=test.index).rename(\"svr_tuned_y_hat\")\n",
    "    \n",
    "    return y_hat_svr\n",
    "\n",
    "def multioutput_svr_tuned_extratrees(train, test, y_keys, best_params_svr, jobs=24):\n",
    "    preds_svr = {}\n",
    "    for y_key in y_keys:\n",
    "        selected_features_svr = extratrees_feature_selection(train, y_key, jobs)\n",
    "        preds_svr[y_key] = svr_tuned(train, test, y_key, selected_features_svr, best_params_svr)\n",
    "    return preds_svr\n",
    "\n",
    "model_config_svr = {\n",
    "    \"tscv_splits\": 20    \n",
    "}\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Perform hyperparameter tuning on the initial training set\n",
    "train_start_size = 200\n",
    "initial_train = data.iloc[:train_start_size]\n",
    "\n",
    "initial_selected_features = {}\n",
    "for y_key in Y_df.columns:\n",
    "    initial_selected_features[y_key] = extratrees_feature_selection(initial_train, y_key, jobs=10)\n",
    "    \n",
    "best_params = {}\n",
    "for y_key in Y_df.columns:\n",
    "    best_params[y_key] = initial_hyperparameter_tuning(initial_train, y_key, initial_selected_features[y_key], model_config_svr)\n",
    "\n",
    "# Initialize empty dictionaries to store results\n",
    "Actuals_svr = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Forecasts_svr = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Forecast_Errors_svr = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "MAE_svr = {}\n",
    "MSE_svr = {}\n",
    "\n",
    "# Expanding window loop\n",
    "for t in range(train_start_size, len(data) - 1):\n",
    "    train = data.iloc[:t]\n",
    "    test = data.iloc[t : t + 1]\n",
    "\n",
    "    preds_svr = multioutput_svr_tuned_extratrees(train, test, Y_df.columns, best_params, jobs=10)\n",
    "\n",
    "    for col in Y_df.columns:\n",
    "        Forecasts_svr[col] = pd.concat([Forecasts_svr[col], preds_svr[col]])\n",
    "        Actuals_svr[col] = pd.concat([Actuals_svr[col], test[col]])\n",
    "        Forecast_Errors_svr[col] = pd.concat([Forecast_Errors_svr[col], test[col] - preds_svr[col]])\n",
    "\n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(data) - 1}\")\n",
    "\n",
    "# Calculate MAE and MSE for each target variable\n",
    "for col in Y_df.columns:\n",
    "    MAE_svr[col] = mean_absolute_error(Actuals_svr[col], Forecasts_svr[col])\n",
    "    MSE_svr[col] = mean_squared_error(Actuals_svr[col], Forecasts_svr[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d787e-4b14-4ccd-9710-be8c764cee3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006d9f3-92fa-4c7e-8837-b9e067e25000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "# Initialize dictionaries for Forecasts_ensemble, Actuals_ensemble, R-squared values and Forecast_Errors_ensemble\n",
    "Forecasts_ensemble = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Actuals_ensemble = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Forecast_Errors_ensemble = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "MAE_ensemble = {}\n",
    "MSE_ensemble = {}\n",
    "R2 = {}\n",
    "\n",
    "for t in range(train_start_size, len(data) - 1):\n",
    "    train = data.iloc[:t]\n",
    "    test = data.iloc[t : t + 1]\n",
    "\n",
    "    preds_ols = multioutput_ols_post_lasso(train, test, Y_df.columns, model_config_ols, jobs=10)\n",
    "    preds_ffn = nn_tuned(train, test, Y_df.columns, best_params_ffn)\n",
    "    preds_svr = multioutput_svr_tuned_extratrees(train, test, Y_df.columns, best_params, jobs=10)\n",
    "\n",
    "    # Ensemble model: Average the predictions of the three models\n",
    "    preds_ensemble = {}\n",
    "    for col in Y_df.columns:\n",
    "        preds_ensemble[col] = (preds_ols[col] + preds_ffn[col] + preds_svr[col]) / 3\n",
    "\n",
    "    for col in Y_df.columns:\n",
    "        Forecasts_ensemble[col] = pd.concat([Forecasts_ensemble[col], preds_ensemble[col]])\n",
    "        Actuals_ensemble[col] = pd.concat([Actuals_ensemble[col], test[col]])\n",
    "        Forecast_Errors_ensemble[col] = pd.concat([Forecast_Errors_ensemble[col], test[col] - preds_ensemble[col]])\n",
    "\n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(data) - 1}\")\n",
    "\n",
    "# Calculate MAE, MSE, and R-squared for each target variable\n",
    "for col in Y_df.columns:\n",
    "    MAE_ensemble[col] = mean_absolute_error(Actuals_ensemble[col], Forecasts_ensemble[col])\n",
    "    MSE_ensemble[col] = mean_squared_error(Actuals_ensemble[col], Forecasts_ensemble[col])\n",
    "    R2[col] = r2_score(Actuals_ensemble[col], Forecasts_ensemble[col])\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    print(f\"Actuals for {col}:\")\n",
    "    print(Actuals_ensemble[col])\n",
    "    print(f\"Forecasts for {col}:\")\n",
    "    print(Forecasts_ensemble[col])\n",
    "    print(f\"Forecast Errors for {col}:\")\n",
    "    print(Forecast_Errors_ensemble[col])\n",
    "    print(f\"Mean Absolute Error for {col}: {MAE_ensemble[col]}\")\n",
    "    print(f\"Mean Squared Error for {col}: {MSE_ensemble[col]}\")\n",
    "    print(f\"R-squared for {col}: {R2[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e79dc-318e-4219-9c24-4757111baf63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9f30f-ebc9-4725-9fa5-88339a4939d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics = pd.DataFrame(columns=['Industry', 'MAE', 'MSE', 'R2'])\n",
    "\n",
    "# Populate the DataFrame with the existing performance metrics\n",
    "for i, col in enumerate(Y_df.columns):\n",
    "    performance_metrics.loc[i, 'Industry'] = col\n",
    "    performance_metrics.loc[i, 'MAE'] = MAE_ensemble[col]\n",
    "    performance_metrics.loc[i, 'MSE'] = MSE_ensemble[col]\n",
    "    performance_metrics.loc[i, 'R2'] = R2[col]\n",
    "\n",
    "# Calculate the mean across all industries and add it\n",
    "mean_mae_ensemble = performance_metrics['MAE'].mean()\n",
    "mean_mse_ensemble = performance_metrics['MSE'].mean()\n",
    "mean_r2_ensemble = performance_metrics['R2'].mean()\n",
    "\n",
    "performance_metrics.loc[len(Y_df.columns), ['Industry', 'MAE', 'MSE', 'R2']] = ['Mean', mean_mae_ensemble, mean_mse_ensemble, mean_r2_ensemble]\n",
    "\n",
    "# Display the performance metrics DataFrame\n",
    "print(performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20b25cb-b908-414b-8fb5-d1e7352f5f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0917c2c-a712-4f8f-bd96-89ba6b646e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Historical_Mean_In_Sample_MSE = {}\n",
    "\n",
    "# Calculate in-sample historical mean forecasts and MSE for each target variable\n",
    "for col in Y_df.columns:\n",
    "    in_sample_data = Y_df.iloc[:train_start_size]  # Use only in-sample data\n",
    "    in_sample_historical_mean = in_sample_data[col].mean()\n",
    "    in_sample_historical_mean_forecast = pd.Series([in_sample_historical_mean] * len(in_sample_data), index=in_sample_data[col].index)\n",
    "    \n",
    "    Historical_Mean_In_Sample_MSE[col] = mean_squared_error(in_sample_data[col], in_sample_historical_mean_forecast)\n",
    "    \n",
    "mean_in_sample_mse = sum(Historical_Mean_In_Sample_MSE.values()) / len(Historical_Mean_In_Sample_MSE)\n",
    "\n",
    "# Create a DataFrame with columns for each performance metric\n",
    "results_df = pd.DataFrame(columns=[\"Historical_Mean_In_Sample_MSE\"])\n",
    "\n",
    "# Put MSE results in to dataframe\n",
    "for col in Y_df.columns:\n",
    "    results_df.loc[col] = [Historical_Mean_In_Sample_MSE[col]]\n",
    "\n",
    "# Calculate and add the mean in-sample MSE\n",
    "results_df.loc[\"Mean\"] = [mean_in_sample_mse]\n",
    "\n",
    "R2_OS_SAE = {}\n",
    "for col in Y_df.columns:\n",
    "    R2_OS_SAE[col] = 1 - (performance_metrics.set_index(\"Industry\").loc[col, \"MSE\"] / results_df.loc[col, \"Historical_Mean_In_Sample_MSE\"])\n",
    "\n",
    "# Create a DataFrame to store the out-of-sample R-squared results\n",
    "out_of_sample_predictability_df = pd.DataFrame(columns=[\"Industry\", \"R2_OS_SAE\"])\n",
    "\n",
    "# put out-of-sample R-squared values in to dataframe\n",
    "for i, col in enumerate(Y_df.columns):\n",
    "    out_of_sample_predictability_df.loc[i, \"Industry\"] = col\n",
    "    out_of_sample_predictability_df.loc[i, \"R2_OS_SAE\"] = R2_OS_SAE[col]\n",
    "\n",
    "# Calculate the mean out-of-sample R-squared across all industries and add it\n",
    "mean_r2_os = out_of_sample_predictability_df[\"R2_OS_SAE\"].mean()\n",
    "out_of_sample_predictability_df.loc[len(Y_df.columns), [\"Industry\", \"R2_OS_SAE\"]] = [\"Mean\", mean_r2_os]\n",
    "\n",
    "print(out_of_sample_predictability_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309009f6-45a6-44d1-ba49-a4a7933f233e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(Actuals_ensemble[col].index, Actuals_ensemble[col].values, label='Actual Returns', linestyle='-', linewidth=1)\n",
    "    plt.plot(Forecasts_ensemble[col].index, Forecasts_ensemble[col].values, label='Forecasted Returns', linestyle='--', linewidth=1)\n",
    "    plt.title(f\"{col} Actual vs. Forecasted Returns\")\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Returns')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe536cf-9e20-4a9f-8c08-6e892f34b5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f31eee-d22d-48e1-a57c-e8dd9e2008a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "train_start_size = 200\n",
    "\n",
    "# Initialize empty dictionaries to store results\n",
    "Actuals_historical = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Historical_Mean_Forecasts = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Historical_Mean_Forecast_Errors = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Historical_Mean_MAE = {}\n",
    "Historical_Mean_MSE = {}\n",
    "Historical_Mean_R2 = {}\n",
    "\n",
    "# Expanding window loop\n",
    "for t in range(train_start_size, len(Y_df) - 1):\n",
    "    train = Y_df.iloc[:t]\n",
    "    test = Y_df.iloc[t : t + 1]\n",
    "\n",
    "    for col in Y_df.columns:\n",
    "        historical_mean = train[col].mean()\n",
    "        historical_mean_forecast = pd.Series([historical_mean], index=test[col].index)\n",
    "        \n",
    "        Historical_Mean_Forecasts[col] = pd.concat([Historical_Mean_Forecasts[col], historical_mean_forecast])\n",
    "        Actuals_historical[col] = pd.concat([Actuals_historical[col], test[col]])\n",
    "        Historical_Mean_Forecast_Errors[col] = pd.concat([Historical_Mean_Forecast_Errors[col], test[col] - historical_mean_forecast])\n",
    "        \n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(Y_df) - 1}\")\n",
    "\n",
    "# Calculate MAE, MSE, and R-squared for each target variable\n",
    "for col in Y_df.columns:\n",
    "    Historical_Mean_MAE[col] = mean_absolute_error(Actuals_historical[col], Historical_Mean_Forecasts[col])\n",
    "    Historical_Mean_MSE[col] = mean_squared_error(Actuals_historical[col], Historical_Mean_Forecasts[col])\n",
    "    Historical_Mean_R2[col] = r2_score(Actuals_historical[col], Historical_Mean_Forecasts[col])\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    print(f\"Actuals for {col}:\")\n",
    "    print(Actuals_historical[col])\n",
    "    print(f\"Historical Mean Forecasts for {col}:\")\n",
    "    print(Historical_Mean_Forecasts[col])\n",
    "    print(f\"Historical Mean Forecast Errors for {col}:\")\n",
    "    print(Historical_Mean_Forecast_Errors[col])\n",
    "    print(f\"Historical Mean MAE for {col}: {Historical_Mean_MAE[col]}\")\n",
    "    print(f\"Historical Mean MSE for {col}: {Historical_Mean_MSE[col]}\")\n",
    "    print(f\"Historical Mean R-squared for {col}: {Historical_Mean_R2[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a234d-408d-4bf1-9328-bbc9f9a71582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a158a6-3496-4e7c-89b6-6622b8800ccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ols(train, test, y_key, selected_features):\n",
    "    X_train = train[selected_features]\n",
    "    X_test = test[selected_features]\n",
    "    y_train = train[y_key]\n",
    "    \n",
    "    ols_model = LinearRegression().fit(X_train, y_train)\n",
    "    y_hat = pd.Series(ols_model.predict(X_test), index=test.index).rename(\"ols_y_hat\")\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "def multioutput_ols(train, test, y_keys):\n",
    "    preds = {}\n",
    "    for y_key in y_keys:\n",
    "        selected_features = train.drop(y_key, axis=1).columns\n",
    "        preds[y_key] = ols(train, test, y_key, selected_features)\n",
    "    return preds\n",
    "\n",
    "Y_diff = Y_df.diff().dropna()\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "train_start_size = 200\n",
    "\n",
    "# Initialize empty dictionaries to store results\n",
    "Actuals_ols = {col: pd.Series(dtype=float, name=col) for col in Y_diff.columns}\n",
    "Forecasts_ols_standard = {col: pd.Series(dtype=float, name=col) for col in Y_diff.columns}\n",
    "Forecast_Errors_ols_standard = {col: pd.Series(dtype=float, name=col) for col in Y_diff.columns}\n",
    "MAE_ols = {}\n",
    "MSE_ols = {}\n",
    "R2_ols = {}\n",
    "\n",
    "# Expanding window loop\n",
    "for t in range(train_start_size, len(data) - 1):\n",
    "    train = data.iloc[:t]\n",
    "    test = data.iloc[t : t + 1]\n",
    "\n",
    "    preds_ols = multioutput_ols(train, test, Y_diff.columns)\n",
    "\n",
    "    for col in Y_diff.columns:\n",
    "        Forecasts_ols_standard[col] = pd.concat([Forecasts_ols_standard[col], preds_ols[col]])\n",
    "        Actuals_ols[col] = pd.concat([Actuals_ols[col], test[col]])\n",
    "        Forecast_Errors_ols_standard[col] = pd.concat([Forecast_Errors_ols_standard[col], test[col] - preds_ols[col]])\n",
    "        \n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(data) - 1}\")\n",
    "\n",
    "# Calculate MAE and MSE for each target variable\n",
    "for col in Y_diff.columns:\n",
    "    MAE_ols[col] = mean_absolute_error(Actuals_ols[col], Forecasts_ols_standard[col])\n",
    "    MSE_ols[col] = mean_squared_error(Actuals_ols[col], Forecasts_ols_standard[col])\n",
    "    R2_ols[col] = r2_score(Actuals_ols[col], Forecasts_ols_standard[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a25a43-1325-4c74-9d36-489e6b0efa36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad46131-1866-4f99-9e7f-13e6b63af66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def diebold_mariano_test(errors1, errors2, h=1, alternative='two_sided'):\n",
    "    \"\"\"\n",
    "    Perform Diebold-Mariano test for equal forecast accuracy.\n",
    "    \n",
    "    errors1 : array_like\n",
    "        Forecast errors from the first model.\n",
    "    errors2 : array_like\n",
    "        Forecast errors from the second model.\n",
    "    h : int, optional\n",
    "        Forecast horizon, default is 1.\n",
    "    alternative : str, optional\n",
    "        Alternative hypothesis, one of 'two_sided', 'greater', 'less', default is 'two_sided'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dm_stat : float\n",
    "        Diebold-Mariano test statistic.\n",
    "    p_value : float\n",
    "        p-value for the Diebold-Mariano test.\n",
    "    \"\"\"\n",
    "    assert len(errors1) == len(errors2), \"Error series must have the same length\"\n",
    "    assert alternative in ['two_sided', 'greater', 'less'], \"Invalid alternative hypothesis\"\n",
    "    \n",
    "    d = errors1**2 - errors2**2\n",
    "    T = len(d)\n",
    "    d_bar = np.mean(d)\n",
    "    gamma0 = np.var(d)\n",
    "    \n",
    "    # Calculate autocovariance of d for lag j\n",
    "    autocov_d = [np.cov(d[:-j], d[j:])[0, 1] for j in range(1, h)]\n",
    "    \n",
    "    # Calculate variance of d_bar\n",
    "    variance_d_bar = (1 / T) * (gamma0 + 2 * sum([((T - j) / T) * autocov_d[j - 1] for j in range(1, h)]))\n",
    "    dm_stat = d_bar / np.sqrt(variance_d_bar)\n",
    "    \n",
    "    if alternative == 'two_sided':\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
    "    elif alternative == 'greater':\n",
    "        p_value = 1 - stats.norm.cdf(dm_stat)\n",
    "    elif alternative == 'less':\n",
    "        p_value = stats.norm.cdf(dm_stat)\n",
    "        \n",
    "    return dm_stat, p_value\n",
    "\n",
    "# Compute forecast errors for each model and target variable\n",
    "errors_ensemble = {col: Forecast_Errors_ensemble[col].values for col in Y_df.columns}\n",
    "errors_ols = {col: Forecast_Errors_ols_standard[col].values for col in Y_df.columns}\n",
    "\n",
    "# Perform Diebold-Mariano test\n",
    "for col in Y_df.columns:\n",
    "    dm_stat, p_value = diebold_mariano_test(errors_ols[col], errors_ensemble[col])\n",
    "    print(f\"Diebold-Mariano test for {col}:\")\n",
    "    print(f\"Test statistic: {dm_stat:.5f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514ded4c-cd6d-416c-bcae-aa1b8165df6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5a486-fe7a-4b47-97ad-ccf797194260",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute forecast errors for each model and target variable\n",
    "errors_svr = {col: Forecast_Errors_svr[col].values for col in Y_df.columns}\n",
    "errors_ols = {col: Forecast_Errors_ols[col].values for col in Y_df.columns}\n",
    "\n",
    "# Perform Diebold-Mariano test\n",
    "for col in Y_df.columns:\n",
    "    dm_stat, p_value = diebold_mariano_test(errors_ols[col], errors_svr[col])\n",
    "    print(f\"Diebold-Mariano test for {col}:\")\n",
    "    print(f\"Test statistic: {dm_stat:.5f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca7e2c-330c-43ab-9639-a5a49c0a8009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f84c5-30a7-49b0-96eb-9b9cd97ad255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute forecast errors for each model and target variable\n",
    "errors_ols_post_lasso = {col: Forecast_Errors_ols[col].values for col in Y_df.columns}\n",
    "errors_ols = {col: Forecast_Errors_ols_standard[col].values for col in Y_df.columns}\n",
    "\n",
    "# Perform Diebold-Mariano test\n",
    "for col in Y_df.columns:\n",
    "    dm_stat, p_value = diebold_mariano_test(errors_ols[col], errors_ols_post_lasso[col])\n",
    "    print(f\"Diebold-Mariano test for {col}:\")\n",
    "    print(f\"Test statistic: {dm_stat:.5f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a67be-ecda-4e53-8d6a-02b315fa087a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea307ed-eccc-4378-b69b-921664e77879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute forecast errors for each model and target variable\n",
    "errors_ffn = {col: Forecast_Errors_ffn[col].values for col in Y_df.columns}\n",
    "errors_ols = {col: Forecast_Errors_ols_standard[col].values for col in Y_df.columns}\n",
    "\n",
    "# Perform Diebold-Mariano test\n",
    "for col in Y_df.columns:\n",
    "    dm_stat, p_value = diebold_mariano_test(errors_ols[col], errors_ffn[col])\n",
    "    print(f\"Diebold-Mariano test for {col}:\")\n",
    "    print(f\"Test statistic: {dm_stat:.5f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86185d3-1a93-4a1e-8000-6b98ffb76f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16887275-7dde-468e-a872-37b9ca77141f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute forecast errors for each model and target variable\n",
    "errors_ols_post_lasso = {col: Forecast_Errors_ols[col].values for col in Y_df.columns}\n",
    "errors_historic = {col: Historical_Mean_Forecast_Errors[col].values for col in Y_df.columns}\n",
    "\n",
    "# Perform Diebold-Mariano test\n",
    "for col in Y_df.columns:\n",
    "    dm_stat, p_value = diebold_mariano_test(errors_historic[col], errors_ols_post_lasso[col])\n",
    "    print(f\"Diebold-Mariano test for {col}:\")\n",
    "    print(f\"Test statistic: {dm_stat:.5f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dec925-906b-420c-b256-b99567e78f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5df4e17-069b-4b7b-b439-607b0b59d419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute forecast errors for each model and target variable\n",
    "errors_ensemble = {col: Forecast_Errors_ensemble[col].values for col in Y_df.columns}\n",
    "errors_historic = {col: Historical_Mean_Forecast_Errors[col].values for col in Y_df.columns}\n",
    "\n",
    "# Perform Diebold-Mariano test\n",
    "for col in Y_df.columns:\n",
    "    dm_stat, p_value = diebold_mariano_test(errors_historic[col], errors_ensemble[col])\n",
    "    print(f\"Diebold-Mariano test for {col}:\")\n",
    "    print(f\"Test statistic: {dm_stat:.5f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc4e78-d6d4-4eff-8267-134c49577e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05452d72-4072-4fc3-9d43-83d58e3dd462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute forecast errors for each model and target variable\n",
    "errors_svr = {col: Forecast_Errors_svr[col].values for col in Y_df.columns}\n",
    "errors_historic = {col: Historical_Mean_Forecast_Errors[col].values for col in Y_df.columns}\n",
    "\n",
    "# Perform Diebold-Mariano test\n",
    "for col in Y_df.columns:\n",
    "    dm_stat, p_value = diebold_mariano_test(errors_historic[col], errors_svr[col])\n",
    "    print(f\"Diebold-Mariano test for {col}:\")\n",
    "    print(f\"Test statistic: {dm_stat:.5f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066b2f5-ec60-4f88-8e7c-9a8801036381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61e5e9-8e8c-46bd-808c-dcfad0917157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute forecast errors for each model and target variable\n",
    "errors_ffn = {col: Forecast_Errors_ffn[col].values for col in Y_df.columns}\n",
    "errors_historic = {col: Historical_Mean_Forecast_Errors[col].values for col in Y_df.columns}\n",
    "\n",
    "# Perform Diebold-Mariano test\n",
    "for col in Y_df.columns:\n",
    "    dm_stat, p_value = diebold_mariano_test(errors_historic[col], errors_ffn[col])\n",
    "    print(f\"Diebold-Mariano test for {col}:\")\n",
    "    print(f\"Test statistic: {dm_stat:.5f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1eb45-9ab5-4a64-b053-0cc83e53f018",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf183c7c-08d4-4f44-b385-771acaf16249",
   "metadata": {},
   "source": [
    "I det sidste afsnit af koden vil der laves en rotationsportef√∏lje for alle modellerne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c26b3-3224-4822-a04c-062cc994282a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230bfa3-549c-42ae-88e8-6b455ef25755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew\n",
    "\n",
    "def calculate_cagr(portfolio_returns):\n",
    "    total_return = np.prod(1 + portfolio_returns)\n",
    "    return (total_return ** (1 / (len(portfolio_returns) / 12))) - 1\n",
    "\n",
    "def calculate_annual_sharpe_ratio(portfolio_returns, risk_free_rate):\n",
    "    excess_returns = portfolio_returns - risk_free_rate / 12\n",
    "    return np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(12)\n",
    "\n",
    "def calculate_mppm(portfolio_returns, risk_free_rate):\n",
    "    excess_returns = portfolio_returns - risk_free_rate / 12\n",
    "    return (1 + np.mean(excess_returns)) / (1 + skew(excess_returns))\n",
    "\n",
    "def calculate_max_drawdown(portfolio_returns):\n",
    "    cum_returns = (1 + portfolio_returns).cumprod()\n",
    "    max_returns = cum_returns.expanding().max()\n",
    "    drawdowns = (cum_returns / max_returns) - 1\n",
    "    return drawdowns.min()\n",
    "\n",
    "def rotation_portfolio_strategy(Forecasts_p, Actuals_p):\n",
    "    portfolio_returns = []\n",
    "\n",
    "    print(\"Month | Long Industries | Short Industries | Monthly Return\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    for t in range(len(Forecasts_p[next(iter(Forecasts_p))])):\n",
    "        forecasts_t = {col: Forecasts_p[col].iloc[t] for col in Forecasts_p}\n",
    "        sorted_industries = sorted(forecasts_t, key=forecasts_t.get)\n",
    "        top_6 = sorted_industries[-6:]\n",
    "        bottom_6 = sorted_industries[:6]\n",
    "\n",
    "        long_positions = sum([Actuals_p[col].iloc[t] / 100 for col in top_6]) / 6\n",
    "        short_positions = sum([-Actuals_p[col].iloc[t] / 100 for col in bottom_6]) / 6\n",
    "\n",
    "        portfolio_return = long_positions - short_positions\n",
    "\n",
    "        portfolio_returns.append(portfolio_return)\n",
    "\n",
    "        # Print chosen industries and the return for the current month (only for the first 12 months)\n",
    "        if t < 100:\n",
    "            print(f\"{t+1:5} | {top_6} | {bottom_6} | {portfolio_return * 100:.2f}%\")\n",
    "\n",
    "    return pd.Series(portfolio_returns, name='Portfolio Returns')\n",
    "\n",
    "# Use the provided risk-free rate\n",
    "risk_free_rate = 0.02\n",
    "\n",
    "years = len(portfolio_returns) / 12\n",
    "\n",
    "Actuals_p = Actuals_ols\n",
    "Forecasts_p = Forecasts_ols\n",
    "\n",
    "# Calculate the portfolio returns using the rotation strategy\n",
    "portfolio_returns = rotation_portfolio_strategy(Forecasts_p, Actuals_p)\n",
    "\n",
    "cagr_ols = calculate_cagr(portfolio_returns)\n",
    "sharpe_ratio_ols = calculate_annual_sharpe_ratio(portfolio_returns, risk_free_rate)\n",
    "mppm_ols = calculate_mppm(portfolio_returns, risk_free_rate)\n",
    "max_drawdown_ols = calculate_max_drawdown(portfolio_returns)\n",
    "years = len(portfolio_returns) / 12\n",
    "\n",
    "# Print results\n",
    "print(f\"CAGR_ols: {cagr_ols * 12:.4f}\")\n",
    "print(f\"Annual Sharpe Ratio ols: {sharpe_ratio_ols:.4f}\")\n",
    "print(f\"MPPM ols: {mppm_ols:.4f}\")\n",
    "print(f\"Maximum Drawdown ols: {max_drawdown_ols:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c3645-b814-430c-8ff0-f863b7be55e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Actuals_p = Actuals_svr\n",
    "Forecasts_p = Forecasts_svr\n",
    "\n",
    "# Calculate the portfolio returns using the rotation strategy\n",
    "portfolio_returns = rotation_portfolio_strategy(Forecasts_p, Actuals_p)\n",
    "\n",
    "cagr_svr = calculate_cagr(portfolio_returns)\n",
    "sharpe_ratio_svr = calculate_annual_sharpe_ratio(portfolio_returns, risk_free_rate)\n",
    "mppm_svr = calculate_mppm(portfolio_returns, risk_free_rate)\n",
    "max_drawdown_svr = calculate_max_drawdown(portfolio_returns)\n",
    "years = len(portfolio_returns) / 12\n",
    "\n",
    "# Print results\n",
    "print(f\"CAGR svr: {cagr_svr * 12:.4f}\")\n",
    "print(f\"Annual Sharpe Ratio svr: {sharpe_ratio_svr:.4f}\")\n",
    "print(f\"MPPM svr: {mppm_svr:.4f}\")\n",
    "print(f\"Maximum Drawdown svr: {max_drawdown_svr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24c8a0-ab58-48a4-aeef-54bc6a4b6ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Actuals_p = Actuals_ffn\n",
    "Forecasts_p = Forecasts_ffn\n",
    "\n",
    "# Calculate the portfolio returns using the rotation strategy\n",
    "portfolio_returns = rotation_portfolio_strategy(Forecasts_p, Actuals_p)\n",
    "\n",
    "cagr_ffn = calculate_cagr(portfolio_returns)\n",
    "sharpe_ratio_ffn = calculate_annual_sharpe_ratio(portfolio_returns, risk_free_rate)\n",
    "mppm_ffn = calculate_mppm(portfolio_returns, risk_free_rate)\n",
    "max_drawdown_ffn = calculate_max_drawdown(portfolio_returns)\n",
    "years = len(portfolio_returns) / 12\n",
    "\n",
    "# Print results\n",
    "print(f\"CAGR ffn: {cagr_ffn * 12:.4f}\")\n",
    "print(f\"Annual Sharpe Ratio ffn: {sharpe_ratio_ffn:.4f}\")\n",
    "print(f\"MPPM ffn: {mppm_ffn:.4f}\")\n",
    "print(f\"Maximum Drawdown ffn: {max_drawdown_ffn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00325f70-b8f2-4298-965b-3a954a491625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c453e1-255a-41f7-98b4-5de545910a55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Actuals_p = Actuals_ols\n",
    "Forecasts_p = Historical_Mean_Forecasts\n",
    "\n",
    "# Calculate the portfolio returns using the rotation strategy\n",
    "portfolio_returns = rotation_portfolio_strategy(Forecasts_p, Actuals_p)\n",
    "\n",
    "cagr_historical = calculate_cagr(portfolio_returns)\n",
    "sharpe_ratio_historical = calculate_annual_sharpe_ratio(portfolio_returns, risk_free_rate)\n",
    "mppm_historical = calculate_mppm(portfolio_returns, risk_free_rate)\n",
    "max_drawdown_historical = calculate_max_drawdown(portfolio_returns)\n",
    "years = len(portfolio_returns) / 12\n",
    "\n",
    "# Print results\n",
    "print(f\"CAGR Historic: {cagr_historical * 12:.4f}\")\n",
    "print(f\"Annual Sharpe Ratio Historic: {sharpe_ratio_historical:.4f}\")\n",
    "print(f\"MPPM Historic: {mppm_historical:.4f}\")\n",
    "print(f\"Maximum Drawdown Historic: {max_drawdown_historical:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc7614-2769-43b8-b426-4b3cc291a0a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Actuals_p = Actuals_ensemble\n",
    "Forecasts_p = Forecasts_ensemble\n",
    "\n",
    "# Calculate the portfolio returns using the rotation strategy\n",
    "portfolio_returns = rotation_portfolio_strategy(Forecasts_p, Actuals_p)\n",
    "\n",
    "cagr_ensemble = calculate_cagr(portfolio_returns)\n",
    "sharpe_ratio_ensemble = calculate_annual_sharpe_ratio(portfolio_returns, risk_free_rate)\n",
    "mppm_ensemble = calculate_mppm(portfolio_returns, risk_free_rate)\n",
    "max_drawdown_ensemble = calculate_max_drawdown(portfolio_returns)\n",
    "years = len(portfolio_returns) / 12\n",
    "\n",
    "# Print results\n",
    "print(f\"CAGR Ensemble: {cagr_ensemble * 12:.4f}\")\n",
    "print(f\"Annual Sharpe Ratio Ensemble: {sharpe_ratio_ensemble:.4f}\")\n",
    "print(f\"MPPM Ensemble: {mppm_ensemble:.4f}\")\n",
    "print(f\"Maximum Drawdown Ensemble: {max_drawdown_ensemble:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbf6970-5ef3-46dd-9277-9708da7fc0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Actuals_p = Actuals_ols\n",
    "Forecasts_p = Forecasts_ols_standard\n",
    "\n",
    "# Calculate the portfolio returns using the rotation strategy\n",
    "portfolio_returns = rotation_portfolio_strategy(Forecasts_p, Actuals_p)\n",
    "\n",
    "cagr_ols_standard = calculate_cagr(portfolio_returns)\n",
    "sharpe_ratio_ols_standard = calculate_annual_sharpe_ratio(portfolio_returns, risk_free_rate)\n",
    "mppm_ols_standard = calculate_mppm(portfolio_returns, risk_free_rate)\n",
    "max_drawdown_ols_standard = calculate_max_drawdown(portfolio_returns)\n",
    "years = len(portfolio_returns) / 12\n",
    "\n",
    "# Print results\n",
    "print(f\"CAGR Ols Standard: {cagr_ols_standard * 12:.4f}\")\n",
    "print(f\"Annual Sharpe Ratio Ols Standard: {sharpe_ratio_ols_standard:.4f}\")\n",
    "print(f\"MPPM Ols Standard: {mppm_ols_standard:.4f}\")\n",
    "print(f\"Maximum Drawdown Ols Standard: {max_drawdown_ols_standard:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97722471-e6bb-46c3-9051-ede58e213242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67996c84-8a1d-42e5-bcc6-e2402862f9ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics_comparison(models, metrics, metric_name):\n",
    "    plt.bar(models, metrics, width=0.4)\n",
    "    plt.title(f'{metric_name} Comparison')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n",
    "\n",
    "# Define the models and their corresponding metrics\n",
    "models = ['OLS', 'SVR', 'FFN', 'Hist Mean', 'Ensemble', 'OLS Std']\n",
    "cagrs = [cagr_ols, cagr_svr, cagr_ffn, cagr_historical, cagr_ensemble, cagr_ols_standard]\n",
    "sharpe_ratios = [sharpe_ratio_ols, sharpe_ratio_svr, sharpe_ratio_ffn, sharpe_ratio_historical, sharpe_ratio_ensemble, sharpe_ratio_ols_standard]\n",
    "mppms = [mppm_ols, mppm_svr, mppm_ffn, mppm_historical, mppm_ensemble, mppm_ols_standard]\n",
    "max_drawdowns = [max_drawdown_ols, max_drawdown_svr, max_drawdown_ffn, max_drawdown_historical, max_drawdown_ensemble, max_drawdown_ols_standard]\n",
    "\n",
    "# Call the function for each metric\n",
    "plot_metrics_comparison(models, cagrs, 'CAGR')\n",
    "plot_metrics_comparison(models, sharpe_ratios, 'Sharpe Ratio')\n",
    "plot_metrics_comparison(models, mppms, 'MPPM')\n",
    "plot_metrics_comparison(models, max_drawdowns, 'Max Drawdown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed349ca2-5125-4d0b-af67-b7cd6987e5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e5bea-f2d1-4505-b290-fae3f5fb4738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_cumulative_returns(portfolio_returns, title):\n",
    "    cum_returns = (1 + portfolio_returns).cumprod() - 1\n",
    "    plt.plot(cum_returns, label=title)\n",
    "\n",
    "# Call the function for each model\n",
    "plot_cumulative_returns(rotation_portfolio_strategy(Forecasts_ols, Actuals_ols), 'OLS')\n",
    "plot_cumulative_returns(rotation_portfolio_strategy(Forecasts_svr, Actuals_svr), 'SVR')\n",
    "plot_cumulative_returns(rotation_portfolio_strategy(Forecasts_ffn, Actuals_ffn), 'FFN')\n",
    "plot_cumulative_returns(rotation_portfolio_strategy(Historical_Mean_Forecasts, Actuals_ols), 'Historical Mean')\n",
    "plot_cumulative_returns(rotation_portfolio_strategy(Forecasts_ensemble, Actuals_ensemble), 'Ensemble')\n",
    "plot_cumulative_returns(rotation_portfolio_strategy(Forecasts_ols_standard, Actuals_ols), 'OLS Standard')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Cumulative Returns of Different Models')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Cumulative Returns')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c80e60-d547-4d66-97b3-bcda24ba4861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_cumulative_returns_in_periods(portfolio_returns, title, start_month, end_month):\n",
    "    cum_returns = (1 + portfolio_returns).cumprod() - 1\n",
    "    plt.plot(cum_returns[start_month:end_month], label=title)\n",
    "\n",
    "def create_subplots(period_title, start_month, end_month):\n",
    "    plot_cumulative_returns_in_periods(rotation_portfolio_strategy(Forecasts_ols, Actuals_ols), 'OLS', start_month, end_month)\n",
    "    plot_cumulative_returns_in_periods(rotation_portfolio_strategy(Forecasts_svr, Actuals_svr), 'SVR', start_month, end_month)\n",
    "    plot_cumulative_returns_in_periods(rotation_portfolio_strategy(Forecasts_ffn, Actuals_ffn), 'FFN', start_month, end_month)\n",
    "    plot_cumulative_returns_in_periods(rotation_portfolio_strategy(Historical_Mean_Forecasts, Actuals_ols), 'Historical Mean', start_month, end_month)\n",
    "    plot_cumulative_returns_in_periods(rotation_portfolio_strategy(Forecasts_ensemble, Actuals_ensemble), 'Ensemble', start_month, end_month)\n",
    "    plot_cumulative_returns_in_periods(rotation_portfolio_strategy(Forecasts_ols_standard, Actuals_ols), 'OLS Standard', start_month, end_month)\n",
    "\n",
    "    plt.title(f'Cumulative Returns of Different Models ({period_title})')\n",
    "    plt.xlabel('Months')\n",
    "    plt.ylabel('Cumulative Returns')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "\n",
    "total_months = len(rotation_portfolio_strategy(Forecasts_ols, Actuals_ols))\n",
    "halfway_month = total_months // 2\n",
    "\n",
    "plt.figure(figsize=(8, 18))\n",
    "\n",
    "# First period\n",
    "plt.subplot(3, 1, 1)\n",
    "create_subplots(\"First Half\", 0, halfway_month)\n",
    "\n",
    "# Second period\n",
    "plt.subplot(3, 1, 2)\n",
    "create_subplots(\"Second Half\", halfway_month, total_months)\n",
    "\n",
    "# Full period\n",
    "plt.subplot(3, 1, 3)\n",
    "create_subplots(\"Full Period\", 0, total_months)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54a599-e58b-4add-8c7f-8e7ab3222685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
