{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aef3e2-8cac-4e4d-a363-5aceb776197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reduce display precision on numpy arrays\n",
    "np.set_printoptions(precision=5)\n",
    "\n",
    "df = pd.read_csv('C:/Users/Computer/Documents/bachelor/dataset.csv')\n",
    "pd.options.mode.chained_assignment=None\n",
    "df['csp'][637:]  = df['csp'][0:638].mean()\n",
    "\n",
    "df['Index'] = pd.to_numeric(df['Index'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1863f-9181-4926-8f38-5fa701a36d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(865)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f676fd4-4a38-4cf9-9a44-0b190e8d12af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check missing values\n",
    "print(f\"Missing values: {df.isna().sum().sum()}\")\n",
    "\n",
    "print(f\"Invalid values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d822c-670a-40a6-b53b-d5a6e358db9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-30].values\n",
    "Y = df.iloc[:, -30:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd7e54-c79d-4cff-bb24-d341570e828c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = df.columns[1:18]\n",
    "target_names = df.columns[-30:]  \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_df = df[feature_names]\n",
    "Y_df = df[target_names]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_df = pd.DataFrame(scaler.fit_transform(X_df), columns=X_df.columns)\n",
    "\n",
    "data = pd.concat([Y_df, X_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59ac65e-3828-4684-8210-5011da1fe76c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV, LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b83133-0931-4472-b74a-df7f9e37479e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lasso_feature_selection(train, test, y_key, model_config, jobs=24):\n",
    "    time_series_cv = TimeSeriesSplit(n_splits=model_config[\"tscv_splits\"])\n",
    "    lasso = LassoCV(n_alphas=model_config[\"n_alphas\"], cv=time_series_cv, n_jobs=jobs, random_state=42, max_iter=20000)\n",
    "    lasso.fit(train.drop(y_key, axis=1), train[y_key])\n",
    "    \n",
    "    selected_features = train.drop(y_key, axis=1).columns[lasso.coef_ != 0]\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144be7f-9b6e-4aab-824c-1ccc9cf0a83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ols(train, test, y_key, selected_features):\n",
    "    X_train = train[selected_features]\n",
    "    X_test = test[selected_features]\n",
    "    y_train = train[y_key]\n",
    "    \n",
    "    ols_model = LinearRegression().fit(X_train, y_train)\n",
    "    y_hat = pd.Series(ols_model.predict(X_test), index=test.index).rename(\"ols_post_lasso_y_hat\")\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886a2f3-f280-4a55-95d0-48a477bdecc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multioutput_ols_post_lasso(train, test, y_keys, model_config, jobs=24):\n",
    "    preds = {}\n",
    "    for y_key in y_keys:\n",
    "        selected_features = lasso_feature_selection(train, test, y_key, model_config, jobs)\n",
    "        preds[y_key] = ols(train, test, y_key, selected_features)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c976719-bdc2-4dc8-97d9-e32c390f0bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"n_alphas\": 100,\n",
    "    \"tscv_splits\": 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b928c6b-e7f2-48d2-8372-3956c07e8006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_current_values_and_errors():\n",
    "    for col in Y_df.columns:\n",
    "        actuals_no_nan = Actuals[col][train_start_size:].dropna()\n",
    "        forecasts_no_nan = Forecasts[col][train_start_size:].dropna()\n",
    "        forecast_errors_no_nan = Forecast_Errors[col][train_start_size:].dropna()\n",
    "        \n",
    "        print(f\"Actuals for {col}:\\n{actuals_no_nan}\")\n",
    "        print(f\"Forecasts for {col}:\\n{forecasts_no_nan}\")\n",
    "        print(f\"Forecast_Errors for {col}:\\n{forecast_errors_no_nan}\")\n",
    "        \n",
    "        mae = mean_absolute_error(actuals_no_nan, forecasts_no_nan)\n",
    "        mse = mean_squared_error(actuals_no_nan, forecasts_no_nan)\n",
    "        \n",
    "        print(f\"Current MAE for {col}: {mae}\")\n",
    "        print(f\"Current MSE for {col}: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d4ded-795f-4ed2-b7c0-dafd4594b602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "train_start_size = 200\n",
    "\n",
    "# Initialize empty dictionaries to store results\n",
    "Actuals = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Forecasts = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Forecast_Errors = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "MAE = {}\n",
    "MSE = {}\n",
    "R2 = {}\n",
    "\n",
    "for t in range(train_start_size, len(data) - 1):\n",
    "    train = data.iloc[:t]\n",
    "    test = data.iloc[t : t + 1]\n",
    "\n",
    "    preds = multioutput_ols_post_lasso(train, test, Y_df.columns, model_config, jobs=10)\n",
    "\n",
    "    for col in Y_df.columns:\n",
    "        Forecasts[col] = pd.concat([Forecasts[col], preds[col]])\n",
    "        Actuals[col] = pd.concat([Actuals[col], test[col]])\n",
    "        Forecast_Errors[col] = pd.concat([Forecast_Errors[col], test[col] - preds[col]])\n",
    "        \n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(data) - 1}\")\n",
    "\n",
    "# Calculate MAE and MSE for each target variable\n",
    "for col in Y_df.columns:\n",
    "    MAE[col] = mean_absolute_error(Actuals[col], Forecasts[col])\n",
    "    MSE[col] = mean_squared_error(Actuals[col], Forecasts[col])\n",
    "    R2[col] = r2_score(Actuals[col], Forecasts[col])\n",
    "\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    print(f\"Actuals for {col}:\")\n",
    "    print(Actuals[col])\n",
    "    print(f\"Forecasts for {col}:\")\n",
    "    print(Forecasts[col])\n",
    "    print(f\"Forecast Errors for {col}:\")\n",
    "    print(Forecast_Errors[col])\n",
    "    print(f\"Mean Absolute Error for {col}: {MAE[col]}\")\n",
    "    print(f\"Mean Squared Error for {col}: {MSE[col]}\")\n",
    "    print(f\"R-squared for {col}: {R2[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a005259-3b28-47eb-9adb-490bfd05dcfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"NaN values in Actuals:\")\n",
    "print(Actuals[col].isna().sum())\n",
    "\n",
    "print(\"NaN values in Forecasts:\")\n",
    "print(Forecasts[col].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9d49b-60e0-4472-b4c7-a6e621d9669f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40d1be-8931-45bb-99bd-7d8a786c43e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "residuals = {col: Actuals[col] - Forecasts[col] for col in Y_df.columns}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=6, ncols=5, figsize=(25, 30))\n",
    "\n",
    "for i, col in enumerate(Y_df.columns):\n",
    "    row_idx = i // 5\n",
    "    col_idx = i % 5\n",
    "    axes[row_idx, col_idx].scatter(Forecasts[col], residuals[col])\n",
    "    axes[row_idx, col_idx].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[row_idx, col_idx].set_xlabel('Predicted Values')\n",
    "    axes[row_idx, col_idx].set_ylabel('Residuals')\n",
    "    axes[row_idx, col_idx].set_title(f'Residuals vs Predicted Values for {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4bd563-2016-4150-bb6b-7be6b47935eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correlation_matrix = data.drop(Y_df.columns, axis=1).corr()\n",
    "\n",
    "# Create a heatmap\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xticks(np.arange(len(correlation_matrix.columns)))\n",
    "ax.set_yticks(np.arange(len(correlation_matrix.index)))\n",
    "ax.set_xticklabels(correlation_matrix.columns)\n",
    "ax.set_yticklabels(correlation_matrix.index)\n",
    "\n",
    "# Rotate the x-axis labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "cbar = fig.colorbar(cax, ax=ax)\n",
    "cbar.ax.set_ylabel(\"Correlation\", rotation=-90, va=\"bottom\")\n",
    "\n",
    "plt.title(\"Heatmap of Predictor Correlations\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba551f-6a88-4612-a922-5a851c14e7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "X = add_constant(data.drop(Y_df.columns, axis=1))\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6dfe6-a2fc-434e-9e4f-225fab3d5778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Calculate the Durbin-Watson statistic for each target variable\n",
    "for col in Y_df.columns:\n",
    "    dw_stat = durbin_watson(Forecast_Errors[col])\n",
    "    print(f\"Durbin-Watson statistic for {col}: {dw_stat:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aaf94e-3e4f-484b-95ab-d02c67795605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "significance_level = 0.05\n",
    "\n",
    "train = data.iloc[:train_start_size]\n",
    "\n",
    "# Calculate in-sample actuals and forecasts\n",
    "in_sample_preds = {}\n",
    "for col in Y_df.columns:\n",
    "    selected_features = lasso_feature_selection(train, train, col, model_config)\n",
    "    in_sample_preds[col] = ols(train, train, col, selected_features)\n",
    "    \n",
    "autocorrelation_results = pd.DataFrame(columns=['Industry', 'Lag', 'Test Statistic', 'p-value', 'Evidence'])\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    actuals = train[col]\n",
    "    residuals = actuals - in_sample_preds[col]\n",
    "    lb_test_result = acorr_ljungbox(residuals, lags=60, return_df=True)\n",
    "\n",
    "    for i, row in lb_test_result.iterrows():\n",
    "        if i + 1 in [2, 12, 24, 36, 48, 60]:  # Only check for lags 2 and 24\n",
    "            lb_stat, p_value = row['lb_stat'], row['lb_pvalue']\n",
    "            evidence = \"Yes\" if p_value < significance_level else \"No\"\n",
    "            result_row = pd.DataFrame(\n",
    "                {\n",
    "                    'Industry': [col],\n",
    "                    'Lag': [i + 1],\n",
    "                    'Test Statistic': [lb_stat],\n",
    "                    'p-value': [p_value],\n",
    "                    'Evidence': [evidence]\n",
    "                }\n",
    "            )\n",
    "            autocorrelation_results = pd.concat([autocorrelation_results, result_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eb2e7b-eac4-4687-b206-b2539943c986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highlight_significant(val):\n",
    "    if val < 0.05:\n",
    "        return 'background-color: green'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Turn the DataFrame to have industries as rows and lags as columns\n",
    "reshaped_autocorrelation_results = autocorrelation_results.pivot_table(\n",
    "    index='Industry',\n",
    "    columns='Lag',\n",
    "    values=['Test Statistic', 'p-value']\n",
    ")\n",
    "\n",
    "# Apply the custom formatting\n",
    "highlighted_pvalues = reshaped_autocorrelation_results['p-value'].style.applymap(highlight_significant)\n",
    "\n",
    "display(highlighted_pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ef382-5f19-4c3c-8635-3441e9a9e010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59937ce-43d8-4663-8eda-a36024e095cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#Breusch-pagan test\n",
    "\n",
    "bp_test_results = {}\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    selected_features = lasso_feature_selection(train, train, col, model_config)\n",
    "    in_sample_forecasts = ols(train, train, col, selected_features)  # Modify this line\n",
    "    \n",
    "    actuals = train[col]\n",
    "    residuals = actuals - in_sample_forecasts\n",
    "    \n",
    "    X_train = train[selected_features]\n",
    "    ols_model = sm.OLS(actuals, sm.add_constant(X_train)).fit()\n",
    "    bp_test_result = het_breuschpagan(ols_model.resid, sm.add_constant(X_train))\n",
    "    bp_test_results[col] = bp_test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f46fb-fb82-4aa0-9db2-f42abf57a7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bp_test_df = pd.DataFrame(columns=[\"Industry\", \"LM Statistic\", \"LM-Test p-value\", \"F-Statistic\", \"F-Test p-value\"])\n",
    "\n",
    "for col, bp_result in bp_test_results.items():\n",
    "    row_df = pd.DataFrame({\"Industry\": [col],\n",
    "                           \"LM Statistic\": [bp_result[0]],\n",
    "                           \"LM-Test p-value\": [bp_result[1]],\n",
    "                           \"F-Statistic\": [bp_result[2]],\n",
    "                           \"F-Test p-value\": [bp_result[3]]})\n",
    "    bp_test_df = pd.concat([bp_test_df, row_df], ignore_index=True)\n",
    "\n",
    "bp_test_df.set_index(\"Industry\", inplace=True)\n",
    "\n",
    "print(\"Breusch-Pagan Test Results:\")\n",
    "print(\"----------------------------\")\n",
    "print(bp_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2439a544-4a36-40b6-be6f-c80f25868917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highlight_bp(val):\n",
    "    if val < 0.05:\n",
    "        return 'background-color: green'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "styled_bp_test_df = bp_test_df.style.applymap(highlight_bp, subset=['LM-Test p-value', 'F-Test p-value'])\n",
    "\n",
    "styled_bp_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ccb76-8a89-40fe-ad21-424367ef3e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract p-values from the results\n",
    "lm_p_values = [result[1] for result in bp_test_results.values()]\n",
    "f_p_values = [result[3] for result in bp_test_results.values()]\n",
    "\n",
    "# Set up the bar chart\n",
    "bar_width = 0.35\n",
    "indices = list(range(len(Y_df.columns)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar1 = ax.bar(indices, lm_p_values, bar_width, label=\"LM-Test p-value\")\n",
    "bar2 = ax.bar([i + bar_width for i in indices], f_p_values, bar_width, label=\"F-Test p-value\")\n",
    "\n",
    "ax.axhline(y=0.05, color='r', linestyle='--')\n",
    "\n",
    "ax.set_xlabel(\"Industries\")\n",
    "ax.set_ylabel(\"p-values\")\n",
    "ax.set_title(\"Breusch-Pagan Test p-values\")\n",
    "ax.set_xticks([i + bar_width / 2 for i in indices])\n",
    "ax.set_xticklabels([str(i) for i in range(1, 31)])\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2462e-977d-46bd-9c99-5ef869b11acd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics = pd.DataFrame(columns=['Industry', 'MAE', 'MSE', 'R2'])\n",
    "\n",
    "# Populate the DataFrame with the existing performance metrics\n",
    "for i, col in enumerate(Y_df.columns):\n",
    "    performance_metrics.loc[i, 'Industry'] = col\n",
    "    performance_metrics.loc[i, 'MAE'] = MAE[col]\n",
    "    performance_metrics.loc[i, 'MSE'] = MSE[col]\n",
    "    performance_metrics.loc[i, 'R2'] = R2[col]\n",
    "\n",
    "# Calculate the mean across all industries\n",
    "mean_mae = performance_metrics['MAE'].mean()\n",
    "mean_mse = performance_metrics['MSE'].mean()\n",
    "mean_r2 = performance_metrics['R2'].mean()\n",
    "\n",
    "performance_metrics.loc[len(Y_df.columns), ['Industry', 'MAE', 'MSE', 'R2']] = ['Mean', mean_mae, mean_mse, mean_r2]\n",
    "\n",
    "print(performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8658bb-8fe4-450b-97c6-e2ba4b4e8200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Historical_Mean_In_Sample_MSE = {}\n",
    "\n",
    "# Calculate in-sample historical mean forecasts and MSE for each target variable\n",
    "for col in Y_df.columns:\n",
    "    in_sample_data = Y_df.iloc[:train_start_size]\n",
    "    in_sample_historical_mean = in_sample_data[col].mean()\n",
    "    in_sample_historical_mean_forecast = pd.Series([in_sample_historical_mean] * len(in_sample_data), index=in_sample_data[col].index)\n",
    "    \n",
    "    Historical_Mean_In_Sample_MSE[col] = mean_squared_error(in_sample_data[col], in_sample_historical_mean_forecast)\n",
    "    \n",
    "mean_in_sample_mse = sum(Historical_Mean_In_Sample_MSE.values()) / len(Historical_Mean_In_Sample_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b764e3c-c436-4c80-b2b0-3c8679df48b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with columns for each performance metric\n",
    "results_df = pd.DataFrame(columns=[\"Historical_Mean_In_Sample_MSE\"])\n",
    "\n",
    "# Put in in-sample MSE results\n",
    "for col in Y_df.columns:\n",
    "    results_df.loc[col] = [Historical_Mean_In_Sample_MSE[col]]\n",
    "\n",
    "# Calculate and add the mean in-sample MSE to the DataFrame\n",
    "results_df.loc[\"Mean\"] = [mean_in_sample_mse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e9d615-aca9-4406-9578-72e41722bbbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "R2_OS_OLS_Post_Lasso = {}\n",
    "for col in Y_df.columns:\n",
    "    R2_OS_OLS_Post_Lasso[col] = 1 - (performance_metrics.set_index(\"Industry\").loc[col, \"MSE\"] / results_df.loc[col, \"Historical_Mean_In_Sample_MSE\"])\n",
    "\n",
    "# Create a DataFrame to store the out-of-sample R-squared results\n",
    "out_of_sample_predictability_df = pd.DataFrame(columns=[\"Industry\", \"R2_OS_OLS_Post_Lasso\"])\n",
    "\n",
    "# Put in the out-of-sample R-squared values\n",
    "for i, col in enumerate(Y_df.columns):\n",
    "    out_of_sample_predictability_df.loc[i, \"Industry\"] = col\n",
    "    out_of_sample_predictability_df.loc[i, \"R2_OS_OLS_Post_Lasso\"] = R2_OS_OLS_Post_Lasso[col]\n",
    "\n",
    "# Calculate the mean out-of-sample R-squared across all industries and add it to the DataFrame\n",
    "mean_r2_os_post_lasso = out_of_sample_predictability_df[\"R2_OS_OLS_Post_Lasso\"].mean()\n",
    "out_of_sample_predictability_df.loc[len(Y_df.columns), [\"Industry\", \"R2_OS_OLS_Post_Lasso\"]] = [\"Mean\", mean_r2_os_post_lasso]\n",
    "\n",
    "print(out_of_sample_predictability_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b31e2-599d-419c-bf3a-f76b3a689499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "train_start_size = 200\n",
    "\n",
    "# Initialize empty dictionaries to store results\n",
    "Actuals = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Historical_Mean_Forecasts = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Historical_Mean_Forecast_Errors = {col: pd.Series(dtype=float, name=col) for col in Y_df.columns}\n",
    "Historical_Mean_MAE = {}\n",
    "Historical_Mean_MSE = {}\n",
    "Historical_Mean_R2 = {}\n",
    "\n",
    "# Expanding window loop\n",
    "for t in range(train_start_size, len(Y_df) - 1):\n",
    "    train = Y_df.iloc[:t]\n",
    "    test = Y_df.iloc[t : t + 1]\n",
    "\n",
    "    for col in Y_df.columns:\n",
    "        historical_mean = train[col].mean()\n",
    "        historical_mean_forecast = pd.Series([historical_mean], index=test[col].index)\n",
    "        \n",
    "        Historical_Mean_Forecasts[col] = pd.concat([Historical_Mean_Forecasts[col], historical_mean_forecast])\n",
    "        Actuals[col] = pd.concat([Actuals[col], test[col]])\n",
    "        Historical_Mean_Forecast_Errors[col] = pd.concat([Historical_Mean_Forecast_Errors[col], test[col] - historical_mean_forecast])\n",
    "        \n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(Y_df) - 1}\")\n",
    "\n",
    "# Calculate MAE, MSE, and R-squared for each target variable\n",
    "for col in Y_df.columns:\n",
    "    Historical_Mean_MAE[col] = mean_absolute_error(Actuals[col], Historical_Mean_Forecasts[col])\n",
    "    Historical_Mean_MSE[col] = mean_squared_error(Actuals[col], Historical_Mean_Forecasts[col])\n",
    "    Historical_Mean_R2[col] = r2_score(Actuals[col], Historical_Mean_Forecasts[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644e367-f7b0-4c92-af4f-ee9f184d3532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bff577-e232-4274-8ba3-441dc9a88792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ols(train, test, y_key, selected_features):\n",
    "    X_train = train[selected_features]\n",
    "    X_test = test[selected_features]\n",
    "    y_train = train[y_key]\n",
    "    \n",
    "    ols_model = LinearRegression().fit(X_train, y_train)\n",
    "    y_hat = pd.Series(ols_model.predict(X_test), index=test.index).rename(\"ols_y_hat\")\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "def multioutput_ols(train, test, y_keys):\n",
    "    preds = {}\n",
    "    for y_key in y_keys:\n",
    "        selected_features = train.drop(y_key, axis=1).columns\n",
    "        preds[y_key] = ols(train, test, y_key, selected_features)\n",
    "    return preds\n",
    "\n",
    "Y_diff = Y_df.diff().dropna()\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "train_start_size = 200\n",
    "\n",
    "# Initialize empty dictionaries to store results\n",
    "Actuals_ols = {col: pd.Series(dtype=float, name=col) for col in Y_diff.columns}\n",
    "Forecasts_ols = {col: pd.Series(dtype=float, name=col) for col in Y_diff.columns}\n",
    "Forecast_Errors_ols = {col: pd.Series(dtype=float, name=col) for col in Y_diff.columns}\n",
    "MAE_ols = {}\n",
    "MSE_ols = {}\n",
    "R2_ols = {}\n",
    "\n",
    "# Expanding window loop\n",
    "for t in range(train_start_size, len(data) - 1):\n",
    "    train = data.iloc[:t]\n",
    "    test = data.iloc[t : t + 1]\n",
    "\n",
    "    preds_ols = multioutput_ols(train, test, Y_diff.columns)\n",
    "\n",
    "    for col in Y_diff.columns:\n",
    "        Forecasts_ols[col] = pd.concat([Forecasts_ols[col], preds_ols[col]])\n",
    "        Actuals_ols[col] = pd.concat([Actuals_ols[col], test[col]])\n",
    "        Forecast_Errors_ols[col] = pd.concat([Forecast_Errors_ols[col], test[col] - preds_ols[col]])\n",
    "        \n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(data) - 1}\")\n",
    "\n",
    "# Calculate MAE and MSE for each target variable\n",
    "for col in Y_diff.columns:\n",
    "    MAE_ols[col] = mean_absolute_error(Actuals_ols[col], Forecasts_ols[col])\n",
    "    MSE_ols[col] = mean_squared_error(Actuals_ols[col], Forecasts_ols[col])\n",
    "    R2_ols[col] = r2_score(Actuals_ols[col], Forecasts_ols[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3785f22-8304-4df2-a82e-6c94d125de28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da91995-58ae-4fb4-b93b-0a913bad53ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def diebold_mariano_test(errors1, errors2, h=1, alternative='two_sided'):\n",
    "    \"\"\"\n",
    "    Perform Diebold-Mariano test for equal forecast accuracy.\n",
    "    \n",
    "    errors1 : array_like\n",
    "        Forecast errors from the first model.\n",
    "    errors2 : array_like\n",
    "        Forecast errors from the second model.\n",
    "    h : int, optional\n",
    "        Forecast horizon, default is 1.\n",
    "    alternative : str, optional\n",
    "        Alternative hypothesis, one of 'two_sided', 'greater', 'less', default is 'two_sided'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dm_stat : float\n",
    "        Diebold-Mariano test statistic.\n",
    "    p_value : float\n",
    "        p-value for the Diebold-Mariano test.\n",
    "    \"\"\"\n",
    "    assert len(errors1) == len(errors2), \"Error series must have the same length\"\n",
    "    assert alternative in ['two_sided', 'greater', 'less'], \"Invalid alternative hypothesis\"\n",
    "    \n",
    "    d = errors1**2 - errors2**2\n",
    "    T = len(d)\n",
    "    d_bar = np.mean(d)\n",
    "    gamma0 = np.var(d)\n",
    "    \n",
    "    # Calculate autocovariance of d for lag j\n",
    "    autocov_d = [np.cov(d[:-j], d[j:])[0, 1] for j in range(1, h)]\n",
    "    \n",
    "    # Calculate variance of d_bar\n",
    "    variance_d_bar = (1 / T) * (gamma0 + 2 * sum([((T - j) / T) * autocov_d[j - 1] for j in range(1, h)]))\n",
    "    dm_stat = d_bar / np.sqrt(variance_d_bar)\n",
    "    \n",
    "    if alternative == 'two_sided':\n",
    "        p_value = 2 * (1 - stats.norm.cdf(abs(dm_stat)))\n",
    "    elif alternative == 'greater':\n",
    "        p_value = 1 - stats.norm.cdf(dm_stat)\n",
    "    elif alternative == 'less':\n",
    "        p_value = stats.norm.cdf(dm_stat)\n",
    "        \n",
    "    return dm_stat, p_value\n",
    "\n",
    "# Compute forecast errors for each model and target variable\n",
    "errors_ols_post_lasso = {col: Forecast_Errors[col].values for col in Y_df.columns}\n",
    "errors_ols = {col: Forecast_Errors_ols[col].values for col in Y_df.columns}\n",
    "\n",
    "# Perform Diebold-Mariano test\n",
    "for col in Y_df.columns:\n",
    "    dm_stat, p_value = diebold_mariano_test(errors_ols[col], errors_ols_post_lasso[col])\n",
    "    print(f\"Diebold-Mariano test for {col}:\")\n",
    "    print(f\"Test statistic: {dm_stat:.5f}\")\n",
    "    print(f\"p-value: {p_value:.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
