{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38a1616-9fec-4564-93b9-df5517e7d04f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reduce display precision on numpy arrays\n",
    "np.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aba2a2-ddaa-4a13-a373-02ca848158b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Computer/Documents/bachelor/dataset.csv')\n",
    "pd.options.mode.chained_assignment=None\n",
    "df['csp'][637:]  = df['csp'][0:638].mean()\n",
    "\n",
    "df['Index'] = pd.to_numeric(df['Index'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7193e565-a3e7-49ec-8782-1459d91776d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86e6e2f-cbe4-45f7-ba3b-c10449516173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:-30].values\n",
    "Y = df.iloc[:, -30:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187d7fb-0bd6-4ca4-8868-a4637405fba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, MultiTaskLassoCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12865c6e-fb7d-4c44-9b41-372c3b7b56f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_names = df.columns[1:18]\n",
    "target_names = df.columns[-30:] \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_df = df[feature_names]\n",
    "Y_df = df[target_names]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_df = pd.DataFrame(scaler.fit_transform(X_df), columns=X_df.columns)\n",
    "\n",
    "data = pd.concat([Y_df, X_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e0109-8a49-4cfb-b4b2-a5819f0a9416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ols(train, test, y_key, selected_features):\n",
    "    X_train = train[selected_features]\n",
    "    X_test = test[selected_features]\n",
    "    y_train = train[y_key]\n",
    "    \n",
    "    ols_model = LinearRegression().fit(X_train, y_train)\n",
    "    y_hat = pd.Series(ols_model.predict(X_test), index=test.index).rename(\"ols_y_hat\")\n",
    "    \n",
    "    return y_hat\n",
    "\n",
    "def multioutput_ols(train, test, y_keys):\n",
    "    preds = {}\n",
    "    for y_key in y_keys:\n",
    "        selected_features = train.drop(y_key, axis=1).columns\n",
    "        preds[y_key] = ols(train, test, y_key, selected_features)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319c8af-b5cb-44a0-b521-2b3db80811ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take the first difference of Y_df to ensure a stationary process\n",
    "Y_diff = Y_df.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc630aa4-3724-4d66-b0c7-deb96d9d0fd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "train_start_size = 200\n",
    "\n",
    "# Initialize empty dictionaries to store results\n",
    "Actuals = {col: pd.Series(dtype=float, name=col) for col in Y_diff.columns}\n",
    "Forecasts = {col: pd.Series(dtype=float, name=col) for col in Y_diff.columns}\n",
    "Forecast_Errors = {col: pd.Series(dtype=float, name=col) for col in Y_diff.columns}\n",
    "MAE = {}\n",
    "MSE = {}\n",
    "R2 = {}\n",
    "\n",
    "# Expanding window loop\n",
    "for t in range(train_start_size, len(data) - 1):\n",
    "    train = data.iloc[:t]\n",
    "    test = data.iloc[t : t + 1]\n",
    "\n",
    "    preds = multioutput_ols(train, test, Y_diff.columns)\n",
    "\n",
    "    for col in Y_diff.columns:\n",
    "        Forecasts[col] = pd.concat([Forecasts[col], preds[col]])\n",
    "        Actuals[col] = pd.concat([Actuals[col], test[col]])\n",
    "        Forecast_Errors[col] = pd.concat([Forecast_Errors[col], test[col] - preds[col]])\n",
    "        \n",
    "    # Print progress every 25 data points\n",
    "    if t % 25 == 0:\n",
    "        print(f\"Progress: {t}/{len(data) - 1}\")\n",
    "\n",
    "# Calculate MAE and MSE for each target variable\n",
    "for col in Y_diff.columns:\n",
    "    MAE[col] = mean_absolute_error(Actuals[col], Forecasts[col])\n",
    "    MSE[col] = mean_squared_error(Actuals[col], Forecasts[col])\n",
    "    R2[col] = r2_score(Actuals[col], Forecasts[col])\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    print(f\"Actuals for {col}:\")\n",
    "    print(Actuals[col])\n",
    "    print(f\"Forecasts for {col}:\")\n",
    "    print(Forecasts[col])\n",
    "    print(f\"Forecast Errors for {col}:\")\n",
    "    print(Forecast_Errors[col])\n",
    "    print(f\"Mean Absolute Error for {col}: {MAE[col]}\")\n",
    "    print(f\"Mean Squared Error for {col}: {MSE[col]}\")\n",
    "    print(f\"R-squared for {col}: {R2[col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d9ea6-cb4a-443e-bfb2-f682df899961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "significance_level = 0.05\n",
    "\n",
    "train = data.iloc[:train_start_size]\n",
    "\n",
    "# Calculate in-sample actuals and forecasts\n",
    "in_sample_preds = multioutput_ols(train, train, Y_df.columns)\n",
    "    \n",
    "autocorrelation_results = pd.DataFrame(columns=['Industry', 'Lag', 'Test Statistic', 'p-value', 'Evidence'])\n",
    "\n",
    "for col in Y_df.columns:\n",
    "    actuals = train[col]\n",
    "    residuals = actuals - in_sample_preds[col]\n",
    "    lb_test_result = acorr_ljungbox(residuals, lags=60, return_df=True)\n",
    "\n",
    "    for i, row in lb_test_result.iterrows():\n",
    "        if i + 1 in [2, 12, 24, 36, 48, 60]:  # Only check for lags 2 and 24\n",
    "            lb_stat, p_value = row['lb_stat'], row['lb_pvalue']\n",
    "            evidence = \"Yes\" if p_value < significance_level else \"No\"\n",
    "            result_row = pd.DataFrame(\n",
    "                {\n",
    "                    'Industry': [col],\n",
    "                    'Lag': [i + 1],\n",
    "                    'Test Statistic': [lb_stat],\n",
    "                    'p-value': [p_value],\n",
    "                    'Evidence': [evidence]\n",
    "                }\n",
    "            )\n",
    "            autocorrelation_results = pd.concat([autocorrelation_results, result_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524946d-a384-41b7-9575-35cb73c511d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highlight_significant(val):\n",
    "    if val < 0.05:\n",
    "        return 'background-color: green'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Pivot the DataFrame to have industries as rows and lags as columns\n",
    "reshaped_autocorrelation_results = autocorrelation_results.pivot_table(\n",
    "    index='Industry',\n",
    "    columns='Lag',\n",
    "    values=['Test Statistic', 'p-value']\n",
    ")\n",
    "\n",
    "# Apply the custom formatting to the p-value part of the reshaped DataFrame\n",
    "highlighted_pvalues = reshaped_autocorrelation_results['p-value'].style.applymap(highlight_significant)\n",
    "\n",
    "# Display the highlighted p-values\n",
    "display(highlighted_pvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4189d13-d731-4d44-a4f4-4fb135f76343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44360c9f-2c3d-4fa1-abca-da3f914872ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Breusch-Pagan test\n",
    "bp_test_results = {}\n",
    "\n",
    "for col in Y_diff.columns:\n",
    "    selected_features = train.drop(col, axis=1).columns\n",
    "    in_sample_forecasts = ols(train, train, col, selected_features)  # Modify this line\n",
    "    \n",
    "    actuals = train[col]\n",
    "    residuals = actuals - in_sample_forecasts\n",
    "    \n",
    "    X_train = train[selected_features]\n",
    "    ols_model = sm.OLS(actuals, sm.add_constant(X_train)).fit()\n",
    "    bp_test_result = het_breuschpagan(ols_model.resid, sm.add_constant(X_train))\n",
    "    bp_test_results[col] = bp_test_result\n",
    "\n",
    "bp_test_df = pd.DataFrame(columns=[\"Industry\", \"LM Statistic\", \"LM-Test p-value\", \"F-Statistic\", \"F-Test p-value\"])\n",
    "\n",
    "for col, bp_result in bp_test_results.items():\n",
    "    row_df = pd.DataFrame({\"Industry\": [col],\n",
    "                           \"LM Statistic\": [bp_result[0]],\n",
    "                           \"LM-Test p-value\": [bp_result[1]],\n",
    "                           \"F-Statistic\": [bp_result[2]],\n",
    "                           \"F-Test p-value\": [bp_result[3]]})\n",
    "    bp_test_df = pd.concat([bp_test_df, row_df], ignore_index=True)\n",
    "\n",
    "# Set the index of the DataFrame to the Industry column\n",
    "bp_test_df.set_index(\"Industry\", inplace=True)\n",
    "\n",
    "# Print the DataFrame as a table\n",
    "print(\"Breusch-Pagan Test Results:\")\n",
    "print(\"----------------------------\")\n",
    "print(bp_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56bdb8e-f6e9-4a54-ab41-1655485f6073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def highlight_bp(val):\n",
    "    if val < 0.05:\n",
    "        return 'background-color: green'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "styled_bp_test_df = bp_test_df.style.applymap(highlight_bp, subset=['LM-Test p-value', 'F-Test p-value'])\n",
    "\n",
    "styled_bp_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84a115-2073-410a-9195-14594cf9d27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27e4c2-6526-47a2-89b3-f355d3a6e447",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "performance_metrics = pd.DataFrame(columns=['Industry', 'MAE', 'MSE', 'R2'])\n",
    "\n",
    "# Populate the DataFrame with the existing performance metrics\n",
    "for i, col in enumerate(Y_df.columns):\n",
    "    performance_metrics.loc[i, 'Industry'] = col\n",
    "    performance_metrics.loc[i, 'MAE'] = MAE[col]\n",
    "    performance_metrics.loc[i, 'MSE'] = MSE[col]\n",
    "    performance_metrics.loc[i, 'R2'] = R2[col]\n",
    "\n",
    "# Calculate the mean across all industries and add it to the DataFrame\n",
    "mean_mae = performance_metrics['MAE'].mean()\n",
    "mean_mse = performance_metrics['MSE'].mean()\n",
    "mean_r2 = performance_metrics['R2'].mean()\n",
    "\n",
    "performance_metrics.loc[len(Y_df.columns), ['Industry', 'MAE', 'MSE', 'R2']] = ['Mean', mean_mae, mean_mse, mean_r2]\n",
    "\n",
    "# Display the performance metrics DataFrame\n",
    "print(performance_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b60e7e-dda9-4194-8b63-a442390b9ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
